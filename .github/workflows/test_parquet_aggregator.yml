name: Aggregate Parquet Data

on:
  workflow_dispatch:
    inputs:
      coordinator_run_id:
        description: 'Coordinator workflow run ID'
        required: true
        type: string
      num_workflows:
        description: 'Number of processor workflows (e.g. 5)'
        required: true
        type: string
        default: '5'

jobs:
  aggregate:
    runs-on: ubuntu-latest
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.10"
          
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pandas pyarrow
          
      - name: Get workflow information
        id: workflow_info
        uses: actions/github-script@v7
        with:
          script: |
            // Get coordinator run info
            const coordinator = await github.rest.actions.getWorkflowRun({
              owner: context.repo.owner,
              repo: context.repo.repo,
              run_id: parseInt('${{ inputs.coordinator_run_id }}')
            });
            
            // Get all workflow runs triggered after coordinator
            const runs = await github.rest.actions.listWorkflowRuns({
              owner: context.repo.owner,
              repo: context.repo.repo,
              workflow_id: 'test_parquet_processor.yml',
              created: `>=${coordinator.data.created_at}`,
              per_page: 100
            });
            
            // Filter runs triggered by this coordinator
            const processorRuns = runs.data.workflow_runs
              .filter(run => run.status === 'completed')
              .filter(run => new Date(run.created_at) >= new Date(coordinator.data.created_at))
              .slice(0, parseInt('${{ inputs.num_workflows }}'));
            
            console.log(`Found ${processorRuns.length} processor runs`);
            
            // Download artifacts from each run
            for (const run of processorRuns) {
              console.log(`Processing run ${run.id} (${run.created_at})`);
              
              const artifacts = await github.rest.actions.listWorkflowRunArtifacts({
                owner: context.repo.owner,
                repo: context.repo.repo,
                run_id: run.id
              });
              
              // Find the parquet data artifact
              const dataArtifact = artifacts.data.artifacts
                .find(a => a.name.startsWith('processed-parquet-data-'));
              
              if (dataArtifact) {
                console.log(`Found artifact: ${dataArtifact.name}`);
                
                // Download the artifact
                const download = await github.rest.actions.downloadArtifact({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  artifact_id: dataArtifact.id,
                  archive_format: 'zip'
                });
                
                // Save to disk
                const fs = require('fs');
                const segmentMatch = dataArtifact.name.match(/processed-parquet-data-(\d+k-\d+k)/);
                const segment = segmentMatch ? segmentMatch[1] : run.id;
                
                fs.mkdirSync(`artifacts/${segment}`, { recursive: true });
                fs.writeFileSync(`artifacts/${segment}/artifact.zip`, Buffer.from(download.data));
                
                console.log(`Downloaded artifact to artifacts/${segment}/artifact.zip`);
              } else {
                console.log('No matching artifact found');
              }
            }
            
      - name: Extract and combine parquet files
        run: |
          python - <<EOF
          import os
          import glob
          import pandas as pd
          from pathlib import Path
          import zipfile
          
          # Extract all zip files
          for segment_dir in glob.glob('artifacts/*'):
              zip_path = os.path.join(segment_dir, 'artifact.zip')
              if os.path.exists(zip_path):
                  print(f"Extracting {zip_path}")
                  with zipfile.ZipFile(zip_path) as zf:
                      zf.extractall(segment_dir)
          
          # Find all parquet files
          all_parquet_files = []
          for segment_dir in glob.glob('artifacts/*'):
              # Recursively find all parquet files
              parquet_files = list(Path(segment_dir).rglob('*.parquet'))
              if parquet_files:
                  # Get the largest parquet file by size
                  largest_file = max(parquet_files, key=lambda p: p.stat().st_size)
                  print(f"Found largest parquet file in {segment_dir}: {largest_file}")
                  all_parquet_files.append(largest_file)
          
          if not all_parquet_files:
              print("No parquet files found!")
              exit(1)
          
          # Combine all parquet files
          print("Combining parquet files...")
          dfs = []
          total_records = 0
          for file in all_parquet_files:
              df = pd.read_parquet(file)
              total_records += len(df)
              dfs.append(df)
              print(f"Read {len(df)} records from {file}")
          
          combined_df = pd.concat(dfs, ignore_index=True)
          print(f"Total records in combined dataset: {len(combined_df)}")
          
          # Save combined dataset
          output_dir = f"data/processed_parquet/combined/{os.environ['GITHUB_RUN_ID']}"
          os.makedirs(output_dir, exist_ok=True)
          output_file = os.path.join(output_dir, f"morphosource_data_combined.parquet")
          combined_df.to_parquet(output_file, index=False)
          print(f"Saved combined dataset to {output_file}")
          
          # Create summary
          with open('aggregation_summary.md', 'w') as f:
              f.write("# Aggregation Summary\n\n")
              f.write(f"- Total records: {len(combined_df)}\n")
              f.write(f"- Files combined: {len(all_parquet_files)}\n")
              f.write(f"- Average processing time: {combined_df['processing_time'].mean():.2f}s\n")
              f.write(f"- Total processing time: {combined_df['processing_time'].sum():.2f}s\n")
              f.write("\n## Source Files:\n")
              for file in all_parquet_files:
                  df = pd.read_parquet(file)
                  f.write(f"- {file}: {len(df)} records\n")
              
              f.write("\n## Media Types:\n")
              f.write(combined_df['media_type'].value_counts().to_string())
          EOF
          
      - name: Upload combined dataset
        uses: actions/upload-artifact@v4
        with:
          name: combined-parquet-data
          path: data/processed_parquet/combined/${{ github.run_id }}
          
      - name: Upload summary
        uses: actions/upload-artifact@v4
        with:
          name: aggregation-summary
          path: aggregation_summary.md 
