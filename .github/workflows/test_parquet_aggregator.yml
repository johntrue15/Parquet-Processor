name: Aggregate Parquet Data

on:
  workflow_dispatch:
    inputs:
      timestamp:
        description: 'Timestamp to aggregate (YYYY-MM-DD_HH-mm-ss)'
        required: true
        type: string

jobs:
  aggregate:
    runs-on: ubuntu-latest
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.10"
          
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pandas pyarrow
          
      - name: Download artifacts
        uses: actions/github-script@v7
        with:
          script: |
            const segments = ['0k-5k', '5k-10k', '10k-15k', '15k-20k', '20k-25k'];
            
            for (const segment of segments) {
              const artifactName = `processed-parquet-data-${segment}-${{ inputs.timestamp }}`;
              console.log(`Downloading artifact: ${artifactName}`);
              
              try {
                const artifacts = await github.rest.actions.listArtifactsForRepo({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  per_page: 100
                });
                
                const artifact = artifacts.data.artifacts.find(a => a.name === artifactName);
                
                if (artifact) {
                  console.log(`Found artifact ${artifactName} (${artifact.id})`);
                  
                  // Download the artifact
                  const download = await github.rest.actions.downloadArtifact({
                    owner: context.repo.owner,
                    repo: context.repo.repo,
                    artifact_id: artifact.id,
                    archive_format: 'zip'
                  });
                  
                  const fs = require('fs');
                  fs.mkdirSync(`artifacts/${segment}`, { recursive: true });
                  fs.writeFileSync(`artifacts/${segment}/artifact.zip`, Buffer.from(download.data));
                  
                  console.log(`Downloaded artifact to artifacts/${segment}/artifact.zip`);
                } else {
                  console.log(`Artifact ${artifactName} not found`);
                }
              } catch (error) {
                console.error(`Error processing ${artifactName}:`, error);
              }
            }
            
      - name: Extract and combine parquet files
        run: |
          python - <<EOF
          import os
          import glob
          import pandas as pd
          from pathlib import Path
          import zipfile
          
          # Extract all zip files
          for segment_dir in glob.glob('artifacts/*k-*k'):
              zip_path = os.path.join(segment_dir, 'artifact.zip')
              if os.path.exists(zip_path):
                  print(f"Extracting {zip_path}")
                  with zipfile.ZipFile(zip_path) as zf:
                      zf.extractall(segment_dir)
          
          # Find all parquet files
          all_parquet_files = []
          for segment_dir in glob.glob('artifacts/*k-*k'):
              # Recursively find all parquet files
              parquet_files = list(Path(segment_dir).rglob('*.parquet'))
              if parquet_files:
                  # Get the largest parquet file by size
                  largest_file = max(parquet_files, key=lambda p: p.stat().st_size)
                  print(f"Found largest parquet file in {segment_dir}: {largest_file}")
                  all_parquet_files.append(largest_file)
          
          if not all_parquet_files:
              print("No parquet files found!")
              exit(1)
          
          # Combine all parquet files
          print("Combining parquet files...")
          dfs = []
          total_records = 0
          for file in all_parquet_files:
              df = pd.read_parquet(file)
              total_records += len(df)
              dfs.append(df)
              print(f"Read {len(df)} records from {file}")
          
          combined_df = pd.concat(dfs, ignore_index=True)
          print(f"Total records in combined dataset: {len(combined_df)}")
          
          # Save combined dataset
          output_dir = f"data/processed_parquet/combined/{inputs.timestamp}"
          os.makedirs(output_dir, exist_ok=True)
          output_file = os.path.join(output_dir, f"morphosource_data_combined.parquet")
          combined_df.to_parquet(output_file, index=False)
          print(f"Saved combined dataset to {output_file}")
          
          # Create summary
          with open('aggregation_summary.md', 'w') as f:
              f.write("# Aggregation Summary\n\n")
              f.write(f"- Total records: {len(combined_df)}\n")
              f.write(f"- Files combined: {len(all_parquet_files)}\n")
              f.write("\n## Source Files:\n")
              for file in all_parquet_files:
                  df = pd.read_parquet(file)
                  f.write(f"- {file}: {len(df)} records\n")
          EOF
          
      - name: Upload combined dataset
        uses: actions/upload-artifact@v4
        with:
          name: combined-parquet-data-${{ inputs.timestamp }}
          path: data/processed_parquet/combined/${{ inputs.timestamp }}
          
      - name: Upload summary
        uses: actions/upload-artifact@v4
        with:
          name: aggregation-summary-${{ inputs.timestamp }}
          path: aggregation_summary.md 
